{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locomotion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Play\n",
    "\n",
    "### 1. Example from repo\n",
    "\n",
    "Extract two sequences from clips and render in matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from mocap.AnimationPlotLines import animation_plot\n",
    "\n",
    "train_data = np.load('mocap/data/edinburgh_locomotion_train.npz')\n",
    "test_data = np.load('mocap/data/edinburgh_locomotion_test.npz')\n",
    "\n",
    "clips = train_data['clips']\n",
    "clips = np.swapaxes(clips, 1, 2)\n",
    "\n",
    "index = 0\n",
    "seq1 = clips[index:index+1]\n",
    "seq2 = clips[index+1:index+2]\n",
    "\n",
    "animation_plot([seq1, seq2], interval=15.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset\n",
    "\n",
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MocapDataset(Dataset):\n",
    "    def __init__(self, path='mocap/data/edinburgh_locomotion_train.npz'):\n",
    "        self.data = torch.from_numpy(np.load(path)['clips'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        joint_positions = self.data[idx][:, :-3]\n",
    "        ctrl = self.data[idx][:, -3:]\n",
    "        return {'j_pos': joint_positions, 'ctrl':ctrl}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DataLoader\n",
    "\n",
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MocapDataLoader(DataLoader):\n",
    "    def __init__(self, dset, batch_size, seq_len=1, overlap_len=0, *args, **kwargs):\n",
    "        super().__init__(dset, batch_size, *args, **kwargs)\n",
    "        self.seq_len = seq_len\n",
    "        self.overlap_len = overlap_len\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in super().__iter__():\n",
    "            jpos = batch['j_pos']\n",
    "            ctrl = batch['ctrl']\n",
    "            batch_size, n_frames, n_joints = jpos.shape\n",
    "\n",
    "            reset = True\n",
    "            for i in range(self.overlap_len, n_frames, self.seq_len):\n",
    "                j = jpos[:, i, :]\n",
    "                c = ctrl[:, i, :]\n",
    "                yield j, c, reset\n",
    "                reset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model\n",
    "\n",
    "Goal - dilated convolutions\n",
    "\n",
    "\n",
    "1d convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=20,\n",
    "                 mid_channels=[20, 10],\n",
    "                 out_size=10):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, mid_channels[0])\n",
    "        self.conv2 = nn.Conv1d(mid_channels[0], mid_channels[1])\n",
    "        self.conv3 = nn.Conv1d(mid_channels[1], mid_channels[2])\n",
    "        self.out = nn.Linear(mid_channels[2], out_size=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
