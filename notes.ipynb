{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locomotion\n",
    "\n",
    "Inspiration: \n",
    "\n",
    "* [Dataset](https://bitbucket.org/jonathan-schwarz/edinburgh_locomotion_mocap_dataset/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play\n",
    "\n",
    "### 1. Example from repo\n",
    "\n",
    "Extract two sequences from\n",
    "clips and render in matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mocap.AnimationPlotLines import animation_plot\n",
    "\n",
    "train_data = np.load('mocap/data/edinburgh_locomotion_train.npz')\n",
    "test_data = np.load('mocap/data/edinburgh_locomotion_test.npz')\n",
    "\n",
    "clips = train_data['clips']\n",
    "clips = np.swapaxes(clips, 1, 2)\n",
    "\n",
    "index = 0\n",
    "seq1 = clips[index:index+1]\n",
    "seq2 = clips[index+1:index+2]\n",
    "\n",
    "animation_plot([seq1, seq2], interval=15.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset\n",
    "\n",
    "There are two items in the dataset `X`- a vector of\n",
    "joint coordinates and `C` - a vector of control signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MocapDataset(Dataset):\n",
    "    def __init__(self, root='mocap/data', train=True):\n",
    "        if train:\n",
    "            dpath = join(root, 'edinburgh_locomotion_train.npz')\n",
    "        else:\n",
    "            dpath = join(root, 'edinburgh_locomotion_test.npz')\n",
    "        self.data = torch.from_numpy(np.load(dpath)['clips'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        joint_positions = self.data[idx][:, :-3]\n",
    "        ctrl = self.data[idx][:, -3:]\n",
    "        return {'j_pos': joint_positions, 'ctrl':ctrl}\n",
    "\n",
    "import random\n",
    "mocap_dataset = MocapDataset()\n",
    "\n",
    "idx = random.randint(0, len(mocap_dataset))\n",
    "d = mocap_dataset[idx]\n",
    "j = d['j_pos']\n",
    "c = d['ctrl']\n",
    "\n",
    "print('Joint shape: ', j.shape)\n",
    "print('Joint mean: ', j.mean())\n",
    "print('Joint min: {}, max: {} '.format(j.min(), j.max()))\n",
    "print('Control shape: ', c.shape)\n",
    "print('Contorl mean: ', c.mean())\n",
    "print('Contorl min: {}, max: {} '.format(c.min(), c.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DataLoader\n",
    "\n",
    "Decide batch size and parallel workers.\n",
    "\n",
    "collate\\_fn\n",
    "\n",
    "Import for efficient training. Keras is easier at these\n",
    "steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MocapDataLoader(DataLoader):\n",
    "    def __init__(self, dset, batch_size, seq_len=1, overlap_len=0, *args, **kwargs):\n",
    "        super().__init__(dset, batch_size, *args, **kwargs)\n",
    "        self.seq_len = seq_len\n",
    "        self.overlap_len = overlap_len\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in super().__iter__():\n",
    "            jpos = batch['j_pos']\n",
    "            ctrl = batch['ctrl']\n",
    "            batch_size, n_frames, n_joints = jpos.shape\n",
    "\n",
    "            reset = True\n",
    "            for i in range(self.overlap_len, n_frames, self.seq_len):\n",
    "                j = jpos[:, i, :]\n",
    "                c = ctrl[:, i, :]\n",
    "                yield j, c, reset\n",
    "                reset = False\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = MocapDataset(mocap_dataset, batch_size, seq_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model\n",
    "\n",
    "Learning:\n",
    "\n",
    "* Dilated convolutions\n",
    "* 1D convolutions"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
